{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "generous-baker",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np \n",
    "from scipy import interp\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pylab as plt\n",
    "from sklearn.metrics import roc_curve,auc\n",
    "import statistics\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "df = pd.read_csv(\"cleve.mod\", sep = \"\\s+\", header=None)\n",
    "df.columns = [\"age\", \"sex\",\"cp\",\"trestbps\",\"chol\",\"fbs\",\"restecg\",\"thalach\",\"exang\",\"oldpeak\",\"slope\",\"ca\",\"thal\",\"class\",\"target\"]\n",
    "\n",
    "#Removing empty values\n",
    "df.isna().sum()\n",
    "\n",
    "#Labelling all the colums in the dataset\n",
    "l1 = LabelEncoder()\n",
    "df = df.apply(l1.fit_transform)\n",
    "\n",
    "#Removing outliers using Inter Quartile Range\n",
    "Q1 = df.quantile(0.25)\n",
    "Q3 = df.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "df = df[~((df < (Q1 - 1.5 * IQR)) |(df > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "\n",
    "\n",
    "x = df.loc[:, df.columns != 'target']\n",
    "y = df.iloc[:,-1:] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "hungarian-frame",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(130, 14)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size = 0.33, random_state = 0)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "talented-label",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling because yes we don't want one independent variable dominating the other and it makes computations easy\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "vietnamese-distinction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 831us/step - loss: -1724.3536 - accuracy: 0.3538\n",
      "Train score: -1724.3536376953125\n",
      "Train accuracy: 0.35384616255760193\n",
      "********************\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -1836.9540 - accuracy: 0.4462\n",
      "Test score: -1836.9539794921875\n",
      "Test accuracy: 0.446153849363327\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "classifier = Sequential()\n",
    "# Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(units = 10, kernel_initializer = 'uniform', activation = 'relu', input_dim = 14))\n",
    "\n",
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(units = 10, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "\n",
    "# Adding the output layer\n",
    "classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
    "\n",
    "# Compiling the ANN | means applying SGD on the whole ANN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Fitting the ANN to the Training set\n",
    "classifier.fit(X_train, Y_train, batch_size = 10, epochs = 100,verbose = 0)\n",
    "\n",
    "score, acc = classifier.evaluate(X_train, Y_train,\n",
    "                            batch_size=10)\n",
    "print('Train score:', score)\n",
    "print('Train accuracy:', acc)\n",
    "# Part 3 - Making predictions and evaluating the model\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)\n",
    "\n",
    "print('*'*20)\n",
    "score, acc = classifier.evaluate(X_test, Y_test,\n",
    "                            batch_size=10)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(Y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "advanced-bernard",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 0.875910 (0.017727)\n",
      "LDA: 0.838215 (0.023684)\n",
      "KNN: 0.961853 (0.011452)\n",
      "CART: 0.980623 (0.008266)\n",
      "NB: 0.665304 (0.029979)\n",
      "SVM: 0.965815 (0.011914)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pylab as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd \n",
    "import sklearn.model_selection as model_selection\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "df = pd.read_csv(\"both_covid_data.csv\")\n",
    "x = df.loc[:, df.columns != 'Class']\n",
    "y = df.iloc[:,-1:]\n",
    "\n",
    "# prepare models\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression()))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC()))\n",
    "\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "scoring=\"accuracy\"\n",
    "for name, model in models:\n",
    "    kfold = model_selection.RepeatedKFold(n_splits=10, n_repeats=5, random_state=1)\n",
    "    cv_results = model_selection.cross_val_score(model, x, y, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    accuracy = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "light-border",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
